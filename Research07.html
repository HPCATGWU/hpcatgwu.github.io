<!DOCTYPE html>
<html  >
<head>
  <!-- Site made with Mobirise Website Builder v5.3.10, https://mobirise.com -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Mobirise v5.3.10, mobirise.com">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
  <link rel="shortcut icon" href="assets/images/screen-shot-2018-12-31-at-12.35.34-128x125.png" type="image/x-icon">
  <meta name="description" content="Website Maker Description">
  
  
  <title>Research07</title>
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons/mobirise-icons.css">
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons-bold/mobirise-icons-bold.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-grid.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-reboot.min.css">
  <link rel="stylesheet" href="assets/tether/tether.min.css">
  <link rel="stylesheet" href="assets/animatecss/animate.min.css">
  <link rel="stylesheet" href="assets/dropdown/css/style.css">
  <link rel="stylesheet" href="assets/socicon/css/styles.css">
  <link rel="stylesheet" href="assets/theme/css/style.css">
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Rubik:300,400,500,600,700,800,900,300i,400i,500i,600i,700i,800i,900i&display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Rubik:300,400,500,600,700,800,900,300i,400i,500i,600i,700i,800i,900i&display=swap"></noscript>
  <link rel="preload" as="style" href="assets/mobirise/css/mbr-additional.css"><link rel="stylesheet" href="assets/mobirise/css/mbr-additional.css" type="text/css">
  
  
  
  
</head>
<body>
  
  <section class="menu cid-rcO6hkGWf0" once="menu" id="menu1-40">

    

    <nav class="navbar navbar-expand beta-menu navbar-dropdown align-items-center navbar-fixed-top navbar-toggleable-sm">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <div class="hamburger">
                <span></span>
                <span></span>
                <span></span>
                <span></span>
            </div>
        </button>
        <div class="menu-logo">
            <div class="navbar-brand">
                <span class="navbar-logo">
                    <a href="Index.html">
                         <img src="assets/images/screen-shot-2018-12-31-at-12.35.34-122x119.png" alt="Mobirise" title="" style="height: 2.9rem;">
                    </a>
                </span>
                <span class="navbar-caption-wrap"><a class="navbar-caption text-black display-2" href="index.html#header1-3">HPCAT<br></a></span>
            </div>
        </div>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav nav-dropdown nav-right" data-app-modern-menu="true"><li class="nav-item">
                    <a class="nav-link link text-black display-4" href="index.html"><span class="mbrib-home mbr-iconfont mbr-iconfont-btn" style="color: rgb(0, 0, 0);"></span>
                        <font size="+1">Home</font></a>
                </li>
                <li class="nav-item">
                    <a class="nav-link link text-black display-4" href="Director.html"><span class="mbrib-user mbr-iconfont mbr-iconfont-btn" style="color: rgb(0, 0, 0);"></span>
                        <font size="+1">Director</font></a>
                </li><li class="nav-item"><a class="nav-link link text-black display-4" href="index.html#content9-23"><span class="mbri-chat mbr-iconfont mbr-iconfont-btn" style="color: rgb(0, 0, 0);"></span>
                        <font size="+1">Research</font></a></li>
                <li class="nav-item"><a class="nav-link link text-black display-4" href="Journal.html"><span class="mbrib-edit mbr-iconfont mbr-iconfont-btn" style="color: rgb(0, 0, 0);"></span>
                        <font size="+1">Publications</font></a>
                </li>
                <li class="nav-item"><a class="nav-link link text-black display-4" href="OurGroup.html"><span class="mbrib-users mbr-iconfont mbr-iconfont-btn" style="color: rgb(0, 0, 0);"></span>
                    <font size="+1">Members</font><br></a>
                </li><li class="nav-item"><a class="nav-link link text-black display-4" href="News.html"><span class="mbrib-preview mbr-iconfont mbr-iconfont-btn" style="color: rgb(0, 0, 0);"></span>
                        <font size="+1">News</font></a></li><li class="nav-item"><a class="nav-link link text-black display-4" href="OpenPositions.html"><span class="mbrib-search mbr-iconfont mbr-iconfont-btn" style="color: rgb(0, 0, 0);"></span>
                        <font size="+1">Open Positions</font></a></li><li class="nav-item"><a class="nav-link link text-black display-4" href="ContactUs.html"><span class="mbrib-letter mbr-iconfont mbr-iconfont-btn" style="color: rgb(0, 0, 0);"></span>
                        <font size="+1">Contact Us</font></a></li></ul>
            
        </div>
    </nav>
</section>

<section class="header1 cid-rcO6hkldkc mbr-parallax-background" id="header1-3z">

    

    

    <div class="container">
        <div class="row justify-content-md-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center mbr-bold pb-3 mbr-fonts-style display-1">
                    Research Topic</h1>
                
                
                
            </div>
        </div>
    </div>

</section>

<section class="mbr-section content4 cid-rcOr29oAum" id="content4-5b">

    

    <div class="container">
        <div class="media-container-row">
            <div class="title col-12 col-md-8">
                <h2 class="align-center pb-3 mbr-fonts-style display-2">Machine Learning Accelerators</h2>
                <h3 class="mbr-section-subtitle align-center mbr-light mbr-fonts-style display-5">Current Researchers: Dr. Hao Zheng, Dr. Ke Wang, Yuan Li, Jiaqi Yang, Juliana Curry, and Jasmine Pillarisetti;</h3>
                
            </div>
        </div>
    </div>
</section>

<section class="mbr-section content6 cid-rCU3FYNSLV" id="content6-9f">
    
     
    
    <div class="container">
        <div class="media-container-row">
            <div class="col-12 col-md-8">
                <div class="media-container-row">
                    <div class="mbr-figure" style="width: 75%;">
                      <img src="assets/images/shutterstock-767334127-676x473.jpeg" alt="Mobirise" title="">  
                    </div>
                    <div class="media-content">
                        <div class="mbr-section-text">
                            <p class="mbr-text mb-0 mbr-fonts-style display-7">Machine learning is currently the foundation for many modern artificial intelligence applications. Since the breakthrough application of deep neural networks (DNNs) to speech recognition and image recognition, the number of applications that use DNNs has exploded. The superior accuracy of DNNs, however, comes at the cost of high computational complexity. While general-purpose compute engines, especially graphics processing units (GPUs), have been the mainstay for much DNN processing, increasingly there is interest in providing more specialized acceleration of the DNN computation. This research project focuses on the development of high-performance and energy-efficient DNN hardware accelerators.</p><p><br></p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-rcOrjTu22r" id="content1-5f">
    
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7"><div></div></div>
        </div>
    </div>
</section>

<section class="mbr-section article content12 cid-thfwxU5Jm7" id="content6-cp">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style pt-5">
                <div class="mb-5">
                    <h4 class="counter">01.</h4>
                    <h3 class="mb-3 mbr-fonts-style display-5">J. Yang, H. Zheng, and A. Louri, "Adapt-Flow: A Flexible DNN Accelerator Design for Heterogeneous Dataflow Implementation", in <em>Proceedings of the ACM/IEEE Great Lakes Symposium on VLSI</em>, Irvine, June 6-8, 2022.</h3>
                    <p class="mbr-fonts-style display-7">Deep neural networks (DNNs) have been widely applied to various application domains. DNN computation is memory and compute-intensive requiring excessive memory access and a large number of computations. To efficiently implement these applications, several data reuse and parallelism exploitation strategies, called dataflows, have been proposed. Studies have shown that many DNN applications benefit from a heterogeneous dataflow strategy where the dataflow type changes from layer to layer. Unfortunately, very few existing DNN architectures can simultaneously accommodate multiple dataflows due to their limited hardware flexibility. In this project, we propose a flexible DNN accelerator architecture, called Adapt-Flow, which has the capability of supporting multiple dataflow selections for each DNN layer at runtime. Specifically, the proposed Adapt-Flow architecture consists of (1) a flexible interconnect, (2) a dataflow selection algorithm, and (3) a dataflow mapping technique. The flexible interconnect provides dynamic support for various traffic patterns required by different dataflows. The proposed dataflow selection algorithm selects the optimal dataflow strategy for a given DNN layer with the aim of much improved performance. And the dataflow mapping technique efficiently maps the dataflow amenable to the flexible interconnect. Simulation studies show that the proposed Adapt-Flow architecture reduces execution time by 46 percent, 78 percent, 26 percent, and energy consumption by 45 percent, 80 percent, 25 percent as compared to NVDLA, ShiDianNao, and Eyeriss respectively.<br></p>
                </div>
              
            </div>
        </div>
    </div>
</section>

<section class="cid-thfwyTOoY8" id="image2-cq">

    

    <figure class="mbr-figure container">
        <div class="image-block" style="width: 55%;">
            <img src="assets/images/jiaqi-1426x675.png" alt="Mobirise" title="">
            
        </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-tld8QsFeIM" id="content6-e6">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style pt-5">
                <div class="mb-5">
                    <h4 class="counter">02.</h4>
                    <h3 class="mb-3 mbr-fonts-style display-5">Y. Li, A. Louri, and A. Karanth, "SPACX: Silicon Photonics-based Scalable Chiplet Accelerator for DNN Inference", in <em>Proceedings of the IEEE International Symposium on High-Performance Computer Architecture (HPCA)</em>, Virtual Conference, April 2-6, 2022.</h3>
                    <p class="mbr-fonts-style display-7">In pursuit of higher inference accuracy, deep neural network (DNN) models have significantly increased in complexity and size. To overcome the consequent computational challenges, scalable chiplet-based accelerators have been proposed. However, data communication using metallic-based interconnects in these chiplet-based DNN accelerators is becoming a primary obstacle to performance, energy efficiency, and scalability. The photonic interconnects can provide adequate data communication support due to some superior properties like low latency, high bandwidth and energy efficiency, and ease of broadcast communication. In this project, we propose SPACX: a Silicon Photonics-based Chiplet Accelerator for DNN inference applications. Specifically, SPACX includes a photonic network design that enables seamless single-chiplet and cross-chiplet broadcast communications, and a tailored dataflow that promotes data broadcast and maximizes parallelism. Furthermore, we explore the broadcast granularities of the photonic network and implications on system performance and energy efficiency. A flexible bandwidth allocation scheme is also proposed to dynamically adjust communication bandwidths for different types of data. Simulation results using several DNN models show that SPACX can achieve 78 percent and 75 percent reduction in execution time and energy, respectively, as compared to other state-of-the-art chiplet-based DNN accelerators.<br></p>
                </div>
              
            </div>
        </div>
    </div>
</section>

<section class="cid-tld8QRexbR" id="image2-e7">

    

    <figure class="mbr-figure container">
        <div class="image-block" style="width: 63%;">
            <img src="assets/images/spacx-1348x632.png" alt="Mobirise" title="">
            
        </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-tld8RwNkiQ" id="content6-e8">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style pt-5">
                <div class="mb-5">
                    <h4 class="counter">03.</h4>
                    <h3 class="mb-3 mbr-fonts-style display-5">Y. Li, A. Louri, and A. Karanth, "Scaling Deep-Learning Inference with Chiplet-based Architecture and Photonic Interconnects", in <em>Proceedings of the Design Automation Conference</em>, San Francisco, CA, December 5-9, 2021.</h3>
                    <p class="mbr-fonts-style display-7">Chiplet-based architectures have been proposed to scale computing systems for deep neural networks (DNNs). Prior work has shown that for the chiplet-based DNN accelerators, the electrical network connecting the chiplets poses a major challenge to system performance, energy consumption, and scalability. Some emerging interconnect technologies such as silicon photonics can potentially overcome the challenges facing electrical interconnects as photonic interconnects provide high bandwidth density, superior energy efficiency, and ease of implementing broadcast and multicast operations that are prevalent in DNN inference. In this project, we propose a chiplet-based architecture named SPRINT for DNN inference. SPRINT uses a global buffer to simplify the data transmission between storage and computation, and includes two novel designs: (1) a reconfigurable photonic network that can support diverse communications in DN inference with minimal implementation cost, and (2) a customized dataflow that exploits the ease of broadcast and multicast feature of photonic interconnects to support highly parallel DNN computations. Simulation studies using ResNet-50 DNN&nbsp; model show&nbsp; that SPRNT achieves 46 percent and 61 percent execution time and energy consumption reduction, respectively, as compared to other state-of-the-art chiplet-based architectures with electrical or photonic interconnects.<br></p>
                </div>
              
            </div>
        </div>
    </div>
</section>

<section class="cid-tld8S2IVWn" id="image2-e9">

    

    <figure class="mbr-figure container">
        <div class="image-block" style="width: 79%;">
            <img src="assets/images/dac-1284x359.png" alt="Mobirise" title="">
            
        </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-tld8SBM9N9" id="content6-ea">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style pt-5">
                <div class="mb-5">
                    <h4 class="counter">04.</h4>
                    <h3 class="mb-3 mbr-fonts-style display-5">K. Shiflett, A. Karanth, A. Louri, and R. Bunescu, "Bitwise Neural Network Acceleration Using Silicon Photonics", in <em>Proceedings of the ACM/IEEE Great Lakes Symposium on VLSI</em>, Virtual Event, June 22-25, 2021.</h3>
                    <p class="mbr-fonts-style display-7">Hardware accelerators provide significant speedup and improve energy efficiency for several demanding deep neural network (DNN) applications. DNNs have several hidden layers that perform concurrent matrix-vector multiplications (MVMs) between the network weights and input features. As MVMs are critical to the performance of DNNs, previous research has optimized the performance and energy efficiency of MVMs at both the architecture and algorithm levels. In this project, we propose to use emerging silicon photonics technology to improve parallelism, speed and overall efficiency with the goal of providing real-time inference and fast training of neural nets. We use microring resonators (MRRs) and Mach Zehnder interferometers (MZIs) to design two versions (all-optical and partial-optical) of hybrid matrix multiplications for DNNs. Our results indicate that our partial optical design gave the best performance in both energy efficiency and latency, with a reduction of 33.1% for energy-delay product (EDP) with conservative estimates and a 76.4% reduction for EDP with aggressive estimates.<br></p>
                </div>
              
            </div>
        </div>
    </div>
</section>

<section class="cid-tld8T241hl" id="image2-eb">

    

    <figure class="mbr-figure container">
        <div class="image-block" style="width: 48%;">
            <img src="assets/images/wechat-screenshot-20221024153634-1325x862.png" alt="Mobirise" title="">
            
        </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-tld8TvtP8J" id="content6-ec">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style pt-5">
                <div class="mb-5">
                    <h4 class="counter">05.</h4>
                    <h3 class="mb-3 mbr-fonts-style display-5">K. Shiflett, A. Karanth, R. Bunescu, and A. Louri, "Scaling Deep-Learning Inference with Chiplet-based Architecture and Photonic Interconnects", in <em>Proceedings of International Symposium on Computer Architecture (ISCA)</em>, Valencia, Spain, June 14-18, 2021.</h3>
                    <p class="mbr-fonts-style display-7">With the end of Dennard scaling, highly-parallel and specialized hardware accelerators have been proposed to improve the throughput and energy-efficiency of deep neural network (DNN) models for various applications. However, collective data movement primitives such as multicast and broadcast that are required for multiply-and-accumulate (MAC) computation in DNN models are expensive, and require excessive energy and latency when implemented with electrical networks. This consequently limits the scalability and performance of electronic hardware accelerators. Emerging technology such as silicon photonics can inherently provide efficient implementation of multicast and broadcast operations, making photonics more amenable to exploit parallelism within DNN models. Moreover, when coupled with other unique features such as low energy consumption, high channel capacity with wavelength-division multiplexing (WDM), and high speed, silicon photonics could potentially provide a viable technology for scaling DNN acceleration.<br>In this work, we propose Albireo, an analog photonic architecture for scaling DNN acceleration. By characterizing photonic devices such as microring resonators (MRRs) and Mach-Zehnder modulators (MZM) using photonic simulators, we develop realistic device models and outline their capability for system level acceleration. Using the device models, we develop an efficient broadcast combined with multicast data distribution by leveraging parameter sharing through unique WDM dot product processing. We evaluate the energy and throughput performance of Albireo on DNN models such as ResNet18, MobileNet and VGG16. When compared to current state-of-the-art electronic accelerators, Albireo increases throughput by 110 X, and improves energy-delay product (EDP) by an average of 74 X with current photonic devices. Furthermore, by considering moderate and aggressive photonic scaling, the proposed Albireo design shows that EDP can be reduced by at least 229 X.<br></p>
                </div>
              
            </div>
        </div>
    </div>
</section>

<section class="cid-tld8TXqo7N" id="image2-ed">

    

    <figure class="mbr-figure container">
        <div class="image-block" style="width: 79%;">
            <img src="assets/images/wechat-screenshot-20221024154142-1731x573.png" alt="Mobirise" title="">
            
        </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-tld8Uutw7w" id="content6-ee">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style pt-5">
                <div class="mb-5">
                    <h4 class="counter">06.</h4>
                    <h3 class="mb-3 mbr-fonts-style display-5">J. Li, A. Louri, A. Karanth, and R. Bunescu, "CSCNN: Algorithm-Hardware Co-Design for CNN Accelerators using Centrosymmetric Filters", in <em>Proceedings of International Symposium on High-Performance Computer Architecture (HPCA)</em>, Seoul, Korea, February 27 - March 3, 2021.</h3>
                    <p class="mbr-fonts-style display-7">Convolutional neural networks (CNNs) are at the core of many state-of-the-art deep learning models in computer vision, speech, and text processing. Training and deploying such CNN-based architectures usually require a significant amount of computational resources. Sparsity has emerged as an effective compression approach for reducing the amount of data and computation for CNNs. However, sparsity often results in computational irregularity, which prevents accelerators from fully taking advantage of its benefits for performance and energy improvement. In this work, we propose CSCNN, an algorithm/hardware co-design framework for CNN compression and acceleration that mitigates the effects of computational irregularity and provides better performance and energy efficiency. On the algorithmic side, CSCNN uses centrosymmetric matrices as convolutional filters. In doing so, it reduces the number of required weights by nearly 50% and enables structured computational reuse without compromising regularity and accuracy. Additionally, complementary pruning techniques are leveraged to further reduce computation by a factor of 2.8-7.2× with a marginal accuracy loss. On the hardware side, we propose a CSCNN accelerator that effectively exploits the structured computational reuse enabled by centrosymmetric filters, and further eliminates zero computations for increased performance and energy efficiency. Compared against a dense accelerator, SCNN and SparTen, the proposed accelerator performs 3.7×, 1.6× and 1.3× better, and improves the EDP (Energy Delay Product) by 8.9×, 2.8× and 2.0×, respectively.<br></p>
                </div>
              
            </div>
        </div>
    </div>
</section>

<section class="cid-tld8UXjMSv" id="image2-ef">

    

    <figure class="mbr-figure container">
        <div class="image-block" style="width: 41%;">
            <img src="assets/images/wechat-screenshot-20221024154552-716x816.png" alt="Mobirise" title="">
            
        </div>
    </figure>
</section>

<section class="mbr-section article content12 cid-tlddeBPGqs" id="content6-eg">
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text counter-container col-12 col-md-8 mbr-fonts-style pt-5">
                <div class="mb-5">
                    <h4 class="counter">07.</h4>
                    <h3 class="mb-3 mbr-fonts-style display-5">K. Shiflett, D. Wright, A. Karanth, and A. Louri, "PIXEL: Photonic Neural Network Accelerator", in <em>Proceedings of the International Symposium on High-Performance Computer Architecture (HPCA)</em>, San Diego, CA, February 22-26, 2020.</h3>
                    <p class="mbr-fonts-style display-7">Machine learning (ML) architectures such as Deep Neural Networks (DNNs) have achieved unprecedented accuracy on modern applications such as image classification and speech recognition. With power dissipation becoming a major concern in ML architectures, computer architects have focused on designing both energy-efficient hardware platforms as well as optimizing ML algorithms. To dramatically reduce power consumption and increase parallelism in neural network accelerators, disruptive technology such as silicon photonics has been proposed which can improve the performance-per-Watt when compared to electrical implementation. In this work, we propose PIXEL - Photonic Neural Network Accelerator that efficiently implements the fundamental operation in neural computation, namely the multiply and accumulate (MAC) functionality using photonic components such as microring
<br>resonators (MRRs) and Mach-Zehnder interferometer (MZI). We design two versions of PIXEL - a hybrid version that multiplies optically and accumulates electrically and a fully optical version that multiplies and accumulates optically. We perform a detailed power, area and timing analysis of the different versions of photonic and electronic accelerators for different convolution neural networks (AlexNet, VGG16, and others). Our results indicate a significant improvement in the energy-delay product for both PIXEL designs over traditional electrical designs (48.4% for OE and 73.9% for OO) while minimizing latency, at the cost of increased area over electrical designs.</p>
                </div>
              
            </div>
        </div>
    </div>
</section>

<section class="cid-tlddf2V6od" id="image2-eh">

    

    <figure class="mbr-figure container">
        <div class="image-block" style="width: 79%;">
            <img src="assets/images/wechat-screenshot-20221024155049-1353x786.png" alt="Mobirise" title="">
            
        </div>
    </figure>
</section>

<section class="cid-rdPq5j4Ss0" id="footer1-8o">

    

    

    <div class="container">
        <div class="media-container-row content text-white">
            <div class="col-12 col-md-3">
                <div class="media-wrap">
                    <a href="#top">
                        <img src="assets/images/screen-shot-2018-12-31-at-12.35.34-310x302.png" alt="Mobirise" title="">
                    </a>
                </div>
            </div>
            <div class="col-12 col-md-5 mbr-fonts-style display-7">
                <h5 class="pb-3"><strong>HPCAT Lab<br>High Performance Computing Architectures &amp; Technologies&nbsp;Lab</strong><br></h5>
                <p class="mbr-text"><strong>Department of Electrical and Computer Enginnering </strong><br><strong>School of Engineering and Applied Science<br>The George Washington University </strong><br><br>800 22nd Street NW<br>Washington, DC 20052<br>United States of America&nbsp;</p>
            </div>
            <div class="col-12 col-md-6 mbr-fonts-style display-7">
                <h5 class="pb-3"><strong>
                    Contact</strong></h5>
                <p class="mbr-text"><strong>Ahmed Louri, IEEE Fellow</strong><br>David and Marilyn Karlgaard Endowed Chair Professor of ECE<br>Director, &nbsp;HPCAT Lab&nbsp;<br><br><br>Email: louri@gwu.edu &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>Phone: +1 (202) 994 8241<br></p>
            </div>
        </div>
        <div class="footer-lower">
            <div class="media-container-row">
                <div class="col-sm-12">
                    <hr>
                </div>
            </div>
            <div class="media-container-row mbr-white">
                <div class="col-sm-6 copyright">
                    <p class="mbr-text mbr-fonts-style display-7">
                        © Copyright 2019 HPCAT - All Rights Reserved
                    </p>
                </div>
                <div class="col-md-6">
                    <div class="social-list align-right">
                        <div class="soc-item">
                            <a href="https://twitter.com/HpcatLab" target="_blank">
                                <span class="mbr-iconfont mbr-iconfont-social socicon-twitter socicon" style="color: rgb(0, 0, 0); fill: rgb(0, 0, 0);"></span>
                            </a>
                        </div>
                        <div class="soc-item">
                            <a href="sorry.html">
                                <span class="mbr-iconfont mbr-iconfont-social socicon-facebook socicon" style="color: rgb(0, 0, 0); fill: rgb(0, 0, 0);"></span>
                            </a>
                        </div>
                        <div class="soc-item">
                            <a href="sorry.html">
                                <span class="mbr-iconfont mbr-iconfont-social socicon-youtube socicon" style="color: rgb(0, 0, 0); fill: rgb(0, 0, 0);"></span>
                            </a>
                        </div>
                        <div class="soc-item">
                            <a href="sorry.html">
                                <span class="mbr-iconfont mbr-iconfont-social socicon-instagram socicon" style="color: rgb(0, 0, 0); fill: rgb(0, 0, 0);"></span>
                            </a>
                        </div>
                        <div class="soc-item">
                            <a href="sorry.html">
                                <span class="mbr-iconfont mbr-iconfont-social socicon-googleplus socicon" style="color: rgb(0, 0, 0); fill: rgb(0, 0, 0);"></span>
                            </a>
                        </div>
                        <div class="soc-item">
                            <a href="sorry.html">
                                <span class="mbr-iconfont mbr-iconfont-social socicon-wechat socicon" style="color: rgb(0, 0, 0); fill: rgb(0, 0, 0);"></span>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section><section style="background-color: #fff; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif; color:#aaa; font-size:12px; padding: 0; align-items: center; display: flex;"><a href="https://mobirise.site/b" style="flex: 1 1; height: 3rem; padding-left: 1rem;"></a><p style="flex: 0 0 auto; margin:0; padding-right:1rem;">Mobirise site creator - <a href="https://mobirise.site/o" style="color:#aaa;">Click for more</a></p></section><script src="assets/web/assets/jquery/jquery.min.js"></script>  <script src="assets/popper/popper.min.js"></script>  <script src="assets/bootstrap/js/bootstrap.min.js"></script>  <script src="assets/tether/tether.min.js"></script>  <script src="assets/smoothscroll/smooth-scroll.js"></script>  <script src="assets/viewportchecker/jquery.viewportchecker.js"></script>  <script src="assets/parallax/jarallax.min.js"></script>  <script src="assets/dropdown/js/nav-dropdown.js"></script>  <script src="assets/dropdown/js/navbar-dropdown.js"></script>  <script src="assets/touchswipe/jquery.touch-swipe.min.js"></script>  <script src="assets/theme/js/script.js"></script>  
  
  
 <div id="scrollToTop" class="scrollToTop mbr-arrow-up"><a style="text-align: center;"><i class="mbr-arrow-up-icon mbr-arrow-up-icon-cm cm-icon cm-icon-smallarrow-up"></i></a></div>
    <input name="animation" type="hidden">
  </body>
</html>